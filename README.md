# 文献智能总结工具

一款带图形界面的本地文献智能总结工具，可以自动处理PDF格式的学术文献，利用大语言模型生成结构化摘要和总体分析报告。

## 功能特性

1. 自动扫描并并行处理指定文件夹中的所有PDF文献
2. 为每篇文献生成结构化摘要（Markdown格式），存储在与原PDF相同的目录下，文件名后缀为 `.summary.md`
3. 生成总体分析报告，包括：
   - 研究主题分布
   - 共性结论
   - 方法对比
   - 待解决问题
4. 在总体报告末尾附上所有文献的详细摘要
5. 支持对单篇文献进行问答对话，具有智能上下文管理功能
6. 文献问答对话框中显示文献总结内容，便于深入了解
7. 图形化界面操作，配置简单直观
8. 支持断点续跑和异常隔离
9. 多种可配置参数，适应不同需求
10. 配置自动保存和加载功能
11. API连接测试功能
12. 可自定义提示词功能
13. 可配置API请求间隔，防止过于频繁调用API

## 安装与配置

### 环境要求

- Python 3.8+
- PyQt5
- 支持OpenAI API格式的LLM服务

### 安装步骤

1. 克隆或下载本项目代码
2. 安装依赖包：
   ```bash
   pip install -r requirements.txt
   ```
   或者分别安装：
   ```bash
   pip install PyQt5 PyPDF2 openai python-dotenv tiktoken
   ```

## 使用说明

详细使用说明请参见 [USAGE.md](USAGE.md) 文件。

1. 启动程序：
   ```bash
   python main.py
   ```

2. 在图形界面中配置：
   - LLM Base URL：LLM服务的API地址
   - API Key：访问LLM服务的认证密钥
   - 模型：要使用的LLM模型名称
   - 文件夹路径：包含PDF文献的文件夹路径
   - 并发数：同时处理的文献数量
   - 最大Token数：单次调用LLM的最大token数
   - API请求间隔：每次API调用之间的等待时间（秒）
   - 其他选项：根据需要选择

3. 点击"测试API连接"按钮验证配置是否正确

4. 点击"开始处理"按钮启动处理流程

5. 处理完成后，可在原文件夹中查看生成的摘要文件（`.summary.md`）和总体报告（`overall_report.md`）

6. 在文献列表中双击任意文献条目，可以打开问答对话窗口，与文献内容进行交互式问答

## 安全注意事项

### 敏感信息保护

本工具会将您的API密钥等敏感配置信息保存在`config.json`文件中。为确保信息安全，请注意以下事项：

1. **Git版本控制**：项目已配置`.gitignore`文件，确保`config.json`不会被提交到代码仓库中
2. **文件权限**：建议设置适当的文件权限，限制对`config.json`文件的访问
3. **备份与同步**：在备份或同步项目时，请注意不要将`config.json`包含在内
4. **共享代码**：在共享项目代码时，请确保不包含`config.json`文件

### 安全建议

1. 定期检查并更新API密钥
2. 使用具有最小必要权限的API密钥
3. 在不需要时关闭程序，避免长时间暴露敏感信息
4. 定期检查生成的文件，确保没有意外包含敏感信息

## 文件存储说明

### 单篇文献摘要文件
每篇文献的摘要以Markdown格式存储，与原始PDF文件位于同一目录下，文件名后缀为 `.summary.md`。
例如：
- 原始文件：`/documents/research_paper.pdf`
- 摘要文件：`/documents/research_paper.summary.md`

### 总体报告文件
处理完所有文献后，会生成一个总体分析报告，存储在程序运行目录下：
- `overall_report.md`

### 原始文本缓存
如果启用了"保留原始文本缓存"选项，提取的PDF文本会缓存在：
- `cache/texts/` 目录下

### 问答记录文件
与文献的问答记录存储在与原始PDF文件相同的目录下，文件名后缀为 `.qa.md`。
例如：
- 原始文件：`/documents/research_paper.pdf`
- 问答记录：`/documents/research_paper.qa.md`

### 自定义提示词文件
用户自定义的提示词存储在：
- `prompts.json`

### 配置文件
敏感配置信息存储在：
- `config.json`（已加入.gitignore，不会被提交到代码仓库）

## 智能上下文管理

本工具具备智能上下文管理功能，能够有效处理长对话历史：

1. **自动Token管理**：系统会自动计算对话历史的Token数量
2. **智能截断机制**：当对话历史过长时，自动截断最早的对话以保持在Token限制内
3. **优先级保护**：系统消息、当前文献内容和最新问题始终保留
4. **历史记录持久化**：对话历史同时保存在内存和磁盘文件中
5. **内存优化**：内存中仅保留最近的对话历史，防止内存占用过大
6. **上下文窗口优化**：避免暴力发送完整历史，通过智能算法优化上下文传递

## 自定义提示词

本工具支持用户自定义提示词，可以通过以下方式修改：

### 使用命令行工具编辑
```bash
python edit_prompts.py
```

该工具提供以下功能：
1. 查看所有提示词
2. 编辑摘要提示词
3. 编辑总体报告提示词
4. 编辑问答提示词
5. 重置指定提示词为默认值
6. 重置所有提示词为默认值

### 直接编辑配置文件
用户也可以直接编辑 `prompts.json` 文件来修改提示词。文件格式如下：
```json
{
  "summary": {
    "system": "系统提示词",
    "user": "用户提示词，包含{text}占位符"
  },
  "overall_report": {
    "system": "系统提示词",
    "user": "用户提示词，包含{summaries}占位符"
  },
  "question_answer": {
    "system": "系统提示词",
    "user": "用户提示词，包含{text}和{question}占位符"
  }
}
```

### 提示词占位符说明
- `{text}`：文献内容占位符（用于摘要和问答）
- `{summaries}`：文献摘要集合占位符（用于总体报告）
- `{question}`：用户问题占位符（用于问答）

## 调试功能

如果遇到问题，可以使用以下调试工具：

### 调试单个PDF文件
```bash
python debug_single_pdf.py path/to/your/file.pdf
```

该工具会：
1. 检查配置是否正确
2. 提取PDF文本并保存为`.debug.txt`文件
3. 生成摘要并保存为`.debug.summary.md`文件
4. 显示详细的处理过程和结果

### 调试PDF提取
```bash
python debug_pdf.py path/to/your/file.pdf
```

### 测试API连接
```bash
python test_api.py <base_url> <api_key> [model]
```

## 配置文件

程序会自动创建并使用`config.json`配置文件来保存用户的设置，避免每次重复输入。配置文件包含以下信息：
- LLM Base URL
- API Key
- 模型名称
- 并发数
- 最大Token数
- API请求间隔
- 是否生成总报告
- 是否保留原始文本缓存
- 文件夹路径

每次启动程序时会自动加载配置，修改配置后点击"保存配置"按钮可将其保存到文件。

## API连接测试

在配置好LLM参数后，可以点击"测试API连接"按钮来验证配置是否正确。测试包括：
1. 验证API连接是否可达
2. 验证API密钥是否有效
3. 验证指定模型是否可用

也可以使用命令行工具进行测试：
```bash
python test_api.py <base_url> <api_key> [model]
```

## 目录结构

```
literature_summary_tool/
├── cache/              # 缓存目录
│   ├── texts/          # 原始文本缓存
│   ├── summaries/      # 摘要缓存
│   └── dialogs/        # 对话缓存
├── core/               # 核心处理模块
├── ui/                 # 图形界面模块
├── utils/              # 工具模块
├── main.py             # 程序入口
├── requirements.txt    # 依赖列表
├── prompts.json        # 自定义提示词配置
├── config.json         # 敏感配置信息（不会被提交到代码仓库）
├── .gitignore          # Git忽略文件列表
└── README.md           # 说明文档
```

## 配置说明

### 必填配置项

- **LLM Base URL**：LLM服务的基础URL，如`http://localhost:8000/v1`
- **API Key**：访问LLM服务的API密钥
- **模型**：要使用的LLM模型名称，如`gpt-3.5-turbo`、`gpt-4`等
- **文件夹路径**：需要处理的PDF文献所在文件夹

### 可选配置项

- **并发数**：同时处理的文献数量（默认5）
- **最大Token数**：单次调用LLM的最大token数（默认2048）
- **API请求间隔**：每次API调用之间的等待时间（秒，默认0）
- **生成总报告**：是否生成总体分析报告（默认开启）
- **保留原始文本缓存**：是否将提取的原始文本保存到cache/texts目录（默认开启）

## 缓存机制

为提高处理效率和避免重复调用LLM，系统采用以下缓存机制：

1. 已生成的摘要文件默认跳过处理（除非强制刷新）
2. 原始文本可选择缓存到`cache/texts`目录
3. 问答对话记录保存在`cache/dialogs`目录

## 断点续跑

程序支持断点续跑功能，默认情况下：
- 已存在摘要文件的文献将被跳过
- 点击"强制刷新"按钮可重新处理所有文献

## 异常处理

程序具有良好的异常隔离机制：
- 单篇文献处理失败不会影响其他文献的处理
- 错误信息会显示在日志区域供用户查看

## 故障排除

### 常见问题

1. **处理失败，成功数为0**：
   - 检查PDF文件是否损坏
   - 确认LLM API配置是否正确
   - 查看详细错误日志以确定具体原因

2. **无法连接到LLM服务**：
   - 检查Base URL是否正确
   - 确认API Key是否有效
   - 检查网络连接是否正常
   - 使用"测试API连接"功能验证配置

3. **提取的文本为空**：
   - 某些PDF文件可能包含扫描图像而非可搜索文本
   - 可以使用`debug_pdf.py`脚本测试单个PDF文件

4. **生成的摘要文件为空**：
   - 可能是LLM返回了空内容
   - 使用`debug_single_pdf.py`脚本进行详细调试
   - 检查LLM服务是否正常工作

5. **提示内容违反使用策略**：
   - 某些文献内容可能触发了LLM的内容安全策略
   - 系统会尝试使用简化提示重试
   - 如果仍然失败，可以尝试手动处理该文献

6. **LLM API调用频率超限**：
   - 降低并发数设置
   - 设置API请求间隔
   - 等待一段时间后重试
   - 检查LLM服务的使用限制

### 调试方法

使用调试脚本测试单个PDF文件：
```bash
python debug_single_pdf.py path/to/your/file.pdf
```

使用调试脚本测试PDF提取：
```bash
python debug_pdf.py path/to/your/file.pdf
```

使用API测试脚本验证LLM配置：
```bash
python test_api.py https://api.openai.com/v1 your-api-key gpt-3.5-turbo
```

## 许可证

本项目采用MIT许可证，详见[LICENSE](LICENSE)文件。