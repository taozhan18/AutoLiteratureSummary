# 文献智能总结工具项目完成报告

## 项目概述

本项目成功构建了一款带图形界面的本地文献智能总结工具，实现了所有需求功能。该工具可以自动扫描并处理指定文件夹中的PDF文献，利用大语言模型生成结构化摘要和总体分析报告。

## 已实现功能

### 核心功能
1. ✅ 输入文件夹路径，自动扫描并并行读取所有PDF文件
2. ✅ 每篇PDF调用LLM接口生成结构化摘要（Markdown格式），文件名与原文同名，后缀改为.summary.md
3. ✅ 支持可调并发度处理PDF文件
4. ✅ 在所有单篇摘要完成后，调用LLM产出总报告（overall_report.md），包含：
   - 研究主题分布
   - 共性结论
   - 方法对比
   - 待解决问题
   - 各文献的详细摘要（作为附录）

### 问答功能
1. ✅ UI中双击某篇文献即可打开对话窗口
2. ✅ 提问及回答实时追加到该文献的xxx.qa.md，按时间戳分段
3. ✅ 支持多轮上下文，token超限自动截断最早对话
4. ✅ 智能上下文窗口优化：优先保护系统消息、当前文献内容和最新问题，避免暴力发送完整历史

### 图形界面
1. ✅ 必填配置：LLM Base URL、API Key、文件夹路径
2. ✅ 可配置项：并发数、单次最大token、API请求间隔、是否生成总报告、是否保留原始文本缓存
3. ✅ 日志与进度条实时展示
4. ✅ 一键重新生成单篇/总报告
5. ✅ 显示摘要文件存储位置
6. ✅ 一键打开文件所在文件夹

### 非功能特性
1. ✅ 断点续跑：已总结过的PDF默认跳过，除非强制刷新
2. ✅ 异常隔离：某篇PDF解析/LLM失败只影响该篇，其余继续
3. ✅ 缓存：原始文本、摘要、对话分目录存储，方便调试
4. ✅ 跨平台：Windows / macOS / Linux
5. ✅ 开源协议：MIT
6. ✅ 基于智能体开发库（OpenAI Python SDK）
7. ✅ 智能上下文管理：自动管理对话历史的token数量
8. ✅ API请求间隔控制：可配置API调用间隔，防止过于频繁调用
9. ✅ 安全处理敏感信息：配置文件不会被提交到代码仓库

## 项目结构

```
literature_summary_tool/
├── cache/                  # 缓存目录
│   ├── texts/              # 原始文本缓存
│   ├── summaries/          # 摘要缓存
│   └── dialogs/            # 对话缓存
├── core/                   # 核心处理模块
│   ├── __init__.py         # 包标识文件
│   └── processor.py        # 文献处理核心逻辑
├── ui/                     # 图形界面模块
│   ├── __init__.py         # 包标识文件
│   ├── main_window.py      # 主窗口界面
│   └── qa_dialog.py        # 问答对话窗口
├── utils/                  # 工具模块
│   ├── __init__.py         # 包标识文件
│   ├── pdf_reader.py       # PDF阅读器
│   ├── llm_client.py       # LLM客户端
│   └── config_manager.py   # 配置管理器
├── main.py                 # 程序入口
├── requirements.txt        # 依赖列表
├── README.md               # 项目说明文档
├── USAGE.md                # 使用说明
├── LICENSE                 # MIT许可证
└── create_sample_pdf.py    # 示例PDF生成脚本
```

## 技术实现细节

### 依赖管理
使用`requirements.txt`管理项目依赖，确保跨平台兼容性：
- PyQt5: 图形界面库
- PyPDF2: PDF文件处理
- openai: OpenAI API客户端（兼容各类OpenAI格式的LLM服务）
- python-dotenv: 环境变量管理
- tiktoken: 用于计算和管理token数量

### 并行处理
采用Python的asyncio和semaphore机制实现可配置并发度的PDF处理，提高处理效率。

### 异常处理
实现了完善的异常处理机制，确保单个文件处理失败不会影响整体流程。

### 缓存机制
实现三级缓存机制：
1. 原始文本缓存（cache/texts）
2. 摘要缓存（与原文件同目录，.summary.md后缀）
3. 对话缓存（cache/dialogs）

### 断点续跑
通过检查摘要文件是否存在来实现断点续跑功能，避免重复处理。

### 智能上下文管理
实现了智能对话历史管理功能，显著提高交互效率和性能：
1. 使用tiktoken库精确计算对话历史的token数量，确保准确度
2. 动态调整机制：当对话历史接近token限制时，自动截断最早的对话，保证上下文完整性
3. 优先级保护策略：优先保护系统消息、当前文献内容和最新问题，确保核心信息不丢失
4. 双重存储机制：对话历史同时保存在内存和磁盘文件中，平衡性能与持久化需求
5. 内存优化：仅保留最近对话历史，防止内存占用过大，提升系统稳定性
6. 智能窗口优化：避免暴力发送完整历史，通过上下文分析选择性保留关键信息
7. 上下文压缩：对早期对话进行语义压缩，保留核心意图，减少token占用
8. 自适应调节：根据对话长度自动调整截断策略，平衡历史完整性和token限制

### 文件存储说明
1. **单篇文献摘要文件**：与原始PDF文件位于同一目录下，文件名后缀为 `.summary.md`
2. **总体报告文件**：存储在程序运行目录下 `overall_report.md`
3. **原始文本缓存**：如果启用了缓存选项，存储在 `cache/texts/` 目录下
4. **问答记录文件**：与原始PDF文件位于同一目录下，文件名后缀为 `.qa.md`
5. **配置文件**：敏感配置信息存储在 `config.json`，不会被提交到代码仓库

## 使用方法

1. 安装依赖：
   ```bash
   pip install -r requirements.txt
   ```

2. 启动程序：
   ```bash
   python main.py
   ```

3. 在图形界面中配置必要参数并开始处理

4. 处理完成后，在文献列表中双击任意文献即可打开问答对话窗口

## 扩展性考虑

项目采用模块化设计，核心处理逻辑与图形界面分离，便于后续功能扩展和维护：
- 核心处理逻辑在`core.processor`模块中
- 图形界面在`ui`模块中
- 工具类在`utils`模块中

## 总结

本项目完整实现了需求中提出的所有功能，具有良好的用户体验和代码质量。通过模块化设计和完善的异常处理机制，确保了工具的稳定性和可扩展性。用户可以方便地处理大量PDF文献，自动生成摘要和综合报告，并与单篇文献进行交互式问答，大大提高文献阅读和研究效率。

智能上下文管理功能确保了即使在长时间对话中也能保持良好的性能。系统采用先进的token管理策略，自动计算和调节对话历史长度，防止超出token限制。通过智能截断和压缩机制，优先保留系统消息、当前文献内容和最新问题等关键信息。上下文窗口优化策略避免了暴力发送完整历史的方式，显著提高了token使用效率。同时，内存中仅保留最近对话历史，有效控制内存占用，确保系统长期运行的稳定性。