# 使用示例

## 快速开始

1. 安装依赖:
   ```bash
   pip install -r requirements.txt
   ```

2. 准备测试数据（可选）:
   ```bash
   # 创建示例PDF文件用于测试
   python create_sample_pdf.py
   ```

3. 启动应用程序:
   ```bash
   python main.py
   ```

## 界面操作指南

### 1. 配置参数

在主界面中填写以下必要参数：

- **LLM Base URL**: 你的大语言模型API地址，例如:
  - OpenAI: `https://api.openai.com/v1`
  - 本地模型(如FastChat): `http://localhost:8000/v1`
  - 阿里云百炼: `https://dashscope.aliyuncs.com/api/v1`
  
- **API Key**: 访问API所需的密钥

- **模型**: 要使用的LLM模型名称，例如:
  - OpenAI: `gpt-3.5-turbo`, `gpt-4`, `gpt-4-turbo`
  - Claude: `claude-3-haiku`, `claude-3-sonnet`, `claude-3-opus`
  - 本地模型: `llama3`, `mixtral` 等
  - 阿里云: `qwen-turbo`, `qwen-plus` 等

- **文件夹路径**: 包含PDF文献的文件夹路径，可以通过"浏览"按钮选择

### 2. 测试API连接

在填写完LLM配置后，强烈建议点击"测试API连接"按钮来验证配置是否正确。测试包括：
1. 验证API服务器是否可达
2. 验证API密钥是否有效
3. 验证指定的模型是否可用

如果测试失败，请检查：
- 网络连接是否正常
- Base URL是否正确
- API密钥是否有效
- 指定的模型是否支持

### 3. 调整可选参数

- **并发数**: 同时处理的文献数量，根据你的系统性能和API限制进行调整
- **最大Token数**: 单次调用LLM时允许的最大token数
- **API请求间隔**: 每次API调用之间的等待时间（秒），用于防止API调用过于频繁
- **生成总报告**: 是否在处理完所有文献后生成总体分析报告
- **保留原始文本缓存**: 是否将提取的PDF文本保存到缓存目录

### 4. 保存配置

点击"保存配置"按钮可以将当前配置保存到`config.json`文件中，下次启动程序时会自动加载这些配置。

### 5. 开始处理

点击"开始处理"按钮，程序将：

1. 扫描指定文件夹中的所有PDF文件
2. 并行处理这些文件，为每篇文献生成摘要
3. 根据配置决定是否生成总体报告
4. 在日志区域显示处理进度和结果
5. 自动保存当前配置

### 6. 查看结果

处理完成后，你可以在以下位置查看结果：

- 每篇文献的摘要文件: 与原PDF文件同目录，文件名后缀为`.summary.md`
- 总体报告: 项目根目录下的`overall_report.md`文件，包含综合分析和各文献的详细摘要
- 原始文本缓存: `cache/texts/`目录（如果启用了缓存）

在程序界面的文献列表中，每项都显示了PDF文件名和对应的摘要文件名，格式为"原文件名 → 摘要文件名"。

### 7. 文献问答

在文献列表中双击任意文献条目，可以打开问答对话窗口：

1. 在输入框中输入关于该文献的问题
2. 按回车或点击"提问"按钮
3. 等待系统基于文献内容生成回答
4. 问答记录会自动保存到与PDF同目录的`.qa.md`文件中

问答功能支持多轮对话，系统会根据之前的对话历史来理解上下文。如果对话历史过长导致token超限，系统会自动截断最早的对话以保持在限制范围内。

## 安全注意事项

### 敏感信息保护

本工具会将您的API密钥等敏感配置信息保存在`config.json`文件中。为确保信息安全，请注意以下事项：

1. **Git版本控制**：项目已配置`.gitignore`文件，确保`config.json`不会被提交到代码仓库中
2. **文件权限**：建议设置适当的文件权限，限制对`config.json`文件的访问
3. **备份与同步**：在备份或同步项目时，请注意不要将`config.json`包含在内
4. **共享代码**：在共享项目代码时，请确保不包含`config.json`文件

### 安全建议

1. 定期检查并更新API密钥
2. 使用具有最小必要权限的API密钥
3. 在不需要时关闭程序，避免长时间暴露敏感信息
4. 定期检查生成的文件，确保没有意外包含敏感信息

### 配置文件示例

为了帮助您正确配置，项目提供了`config.json.example`文件作为配置模板。您可以参考此文件创建自己的`config.json`文件：

```bash
# 复制示例配置文件
cp config.json.example config.json

# 然后编辑config.json文件，填入您的实际配置信息
```

请注意，`config.json.example`文件中不包含任何真实的敏感信息，您需要将其替换为您自己的配置信息。

## 智能上下文管理

本工具具备智能上下文管理功能，能够有效处理长对话历史：

### 自动Token管理
系统使用`tiktoken`库来精确计算对话历史的Token数量，确保不会超过LLM的最大Token限制。

### 智能截断机制
当对话历史接近Token限制时，系统会自动执行以下操作：
1. 保留系统消息（角色设定）
2. 保留当前文献内容和用户问题
3. 从最早的对话历史开始逐步移除，直到满足Token限制

### 优先级保护
在上下文管理中，以下内容具有最高优先级，永远不会被截断：
- 系统角色设定消息
- 当前文献内容
- 当前用户问题

### 历史记录持久化
对话历史同时保存在以下两个位置：
1. 内存中：用于当前对话的上下文传递（限制为最近20轮对话）
2. 磁盘文件：与PDF文件同目录的`.qa.md`文件，用于持久化存储

### 上下文窗口优化
与传统的将完整历史发送给LLM的方式不同，本工具采用上下文窗口优化策略：
1. 避免暴力发送完整历史，减少token浪费
2. 通过智能算法优化上下文传递，保持对话连贯性
3. 根据token限制动态调整上下文内容

## 自定义提示词

本工具支持用户自定义提示词，以满足不同场景和偏好需求。

### 提示词类型

工具包含三种主要提示词类型：

1. **摘要提示词** (`summary`)：用于生成单篇文献摘要
2. **总体报告提示词** (`overall_report`)：用于生成多篇文献的综合分析报告
3. **问答提示词** (`question_answer`)：用于文献问答功能

### 使用命令行工具编辑提示词

运行以下命令启动提示词编辑工具：

```bash
python edit_prompts.py
```

工具界面将显示以下选项：
1. 查看所有提示词
2. 编辑摘要提示词
3. 编辑总体报告提示词
4. 编辑问答提示词
5. 重置指定提示词为默认值
6. 重置所有提示词为默认值

### 直接编辑配置文件

用户也可以直接编辑 `prompts.json` 文件来自定义提示词。配置文件采用标准JSON格式：

```json
{
  "summary": {
    "system": "你是一位专业的学术文献分析师，能够准确提取和总结文献的核心内容。",
    "user": "请为以下学术文献生成一个结构化摘要，使用Markdown格式输出：\n\n1. 研究背景与目标\n2. 方法论\n3. 主要发现\n4. 结论与意义\n5. 局限性\n\n文献内容：\n{text}"
  },
  "overall_report": {
    "system": "你是一位专业的学术研究报告撰写专家，能够综合分析多篇文献并产出深度分析报告。",
    "user": "基于以下多篇文献的摘要，生成一份总体报告，包括：\n\n1. 研究主题分布\n2. 共性结论\n3. 方法对比\n4. 待解决问题\n\n文献摘要：\n{summaries}"
  },
  "question_answer": {
    "system": "你是一位专业的学术助手，能够基于文献内容准确回答用户问题。",
    "user": "文献内容：\n{text}\n\n问题：\n{question}"
  }
}
```

### 提示词占位符说明

在自定义提示词时，可以使用以下占位符：

- `{text}`：文献内容占位符（用于摘要和问答）
- `{summaries}`：文献摘要集合占位符（用于总体报告）
- `{question}`：用户问题占位符（用于问答）

占位符会在运行时被实际内容替换。

### 自定义提示词示例

#### 1. 更详细的摘要提示词
```json
{
  "summary": {
    "system": "你是一位专业的学术文献分析师，能够准确提取和总结文献的核心内容。",
    "user": "请为以下学术文献生成一个详细的结构化摘要，使用Markdown格式输出：\n\n## 研究背景与目标\n描述研究的背景、动机和具体目标\n\n## 理论基础\n涉及的主要理论和概念\n\n## 研究方法\n详细说明采用的研究方法、实验设计、数据收集和分析方法\n\n## 主要发现\n研究的核心发现和结果\n\n## 结论与意义\n研究的主要结论及其理论和实践意义\n\n## 局限性与未来工作\n研究的局限性和未来可能的研究方向\n\n文献内容：\n{text}"
  }
}
```

#### 2. 中文文献摘要提示词
```json
{
  "summary": {
    "system": "你是一位专业的中文学术文献分析师，能够准确提取和总结中文文献的核心内容。",
    "user": "请为以下中文学术文献生成一个结构化摘要，使用Markdown格式输出：\n\n一、研究背景与目标\n二、研究方法\n三、主要发现\n四、结论与意义\n五、局限性\n\n文献内容：\n{text}"
  }
}
```

### 重置提示词

如果自定义提示词导致问题或想要恢复默认设置，可以通过以下方式重置：

1. 使用命令行工具选择重置选项
2. 删除 `prompts.json` 文件，程序会自动生成默认配置
3. 在命令行工具中选择"重置所有提示词为默认值"

## API请求间隔

为了避免过于频繁地调用API，可以设置API请求间隔：

### 为什么需要API请求间隔

1. **API速率限制**：许多LLM服务对API调用频率有限制
2. **资源保护**：防止短时间内大量请求导致服务不稳定
3. **成本控制**：减少不必要的API调用，控制使用成本
4. **稳定性提升**：适当的间隔可以提高整体处理的稳定性

### 如何设置API请求间隔

在主界面的配置区域，可以找到"API请求间隔"选项，设置每次API调用之间需要等待的秒数。

- **0秒**：无间隔，立即进行下一次API调用（默认）
- **1-60秒**：在每次API调用之间等待指定的秒数

### 使用建议

1. **无限制API**：如果使用的是本地部署的模型或者没有速率限制的API，可以保持为0
2. **有限制API**：如果是使用有速率限制的API（如OpenAI、阿里云等），建议设置适当的间隔
3. **高并发场景**：在高并发处理大量文献时，建议增加API请求间隔以避免触发限制
4. **测试环境**：在测试时可以设置较短的间隔，生产环境建议根据API限制调整

## 文件存储说明

### 单篇文献摘要文件
每篇文献的摘要以Markdown格式存储，与原始PDF文件位于同一目录下，文件名后缀为 `.summary.md`。
例如：
- 原始文件：`/documents/research_paper.pdf`
- 摘要文件：`/documents/research_paper.summary.md`

### 总体报告文件
处理完所有文献后，会生成一个总体分析报告，存储在程序运行目录下：
- `overall_report.md`

### 原始文本缓存
如果启用了"保留原始文本缓存"选项，提取的PDF文本会缓存在：
- `cache/texts/` 目录下

### 问答记录文件
与文献的问答记录存储在与原始PDF文件相同的目录下，文件名后缀为 `.qa.md`。
例如：
- 原始文件：`/documents/research_paper.pdf`
- 问答记录：`/documents/research_paper.qa.md`

### 自定义提示词文件
用户自定义的提示词存储在：
- `prompts.json`

### 配置文件
敏感配置信息存储在：
- `config.json`（不会被提交到代码仓库）

## 常见问题

### Q: 如何处理已存在的摘要文件？

默认情况下，程序会跳过已存在摘要文件的文献。如果需要重新生成所有摘要，可以点击"强制刷新"按钮。

### Q: 如何处理处理失败的文献？

程序具有异常隔离机制，单篇文献处理失败不会影响其他文献的处理。失败信息会显示在日志区域。常见的失败原因包括：

1. **文献文本提取失败**：PDF文件可能包含扫描图像而非可搜索文本
2. **LLM返回空内容**：可能是文献内容过短或格式不符合预期
3. **API连接问题**：网络连接或API密钥问题
4. **内容政策违规**：某些文献内容可能触发了LLM的内容安全策略

### Q: 支持哪些大语言模型？

本工具支持所有兼容OpenAI API格式的大语言模型，包括：

- OpenAI GPT系列
- Anthropic Claude系列
- 本地部署的模型（如通过FastChat部署）
- 阿里云、百度云等云服务商的模型API

你可以在模型下拉框中选择预设的模型，也可以手动输入你使用的模型名称。

### Q: 如何提高处理速度？

可以通过以下方式提高处理速度：

1. 增加并发数（根据系统性能和API限制调整）
2. 使用性能更强的本地模型
3. 启用缓存避免重复处理
4. 适当调整API请求间隔，在保证稳定性的前提下尽可能减少等待时间

### Q: 为什么某些文献处理失败？

文献处理失败可能有以下原因：

1. **文献内容过短**：少于100字符的文本可能不是有效的学术文献
2. **内容政策违规**：某些文献内容可能触发了LLM的内容安全策略
3. **API限制**：如频率限制、内容长度限制等
4. **网络问题**：临时网络连接问题

对于内容政策违规的情况，系统会尝试使用简化提示重试。如果仍然失败，建议使用调试工具单独处理该文献。

### Q: 如何调试处理失败的文献？

可以使用以下方法调试处理失败的文献：

1. 使用`debug_single_pdf.py`脚本进行详细调试：
   ```bash
   python debug_single_pdf.py path/to/failed/file.pdf
   ```

2. 查看日志中的详细错误信息

3. 检查生成的`.debug.txt`文件，确认提取的文本是否正确

4. 检查生成的`.debug.summary.md`文件，确认摘要是否正确生成